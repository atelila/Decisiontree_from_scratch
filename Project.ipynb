{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "796658f6-833f-48d6-8073-670c461f420f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from sklearn import tree\n",
    "\n",
    "data = pd.read_csv(\"wine_dataset.csv\") \n",
    "\n",
    "features = data.loc[:,: \"alcohol\"] \n",
    "labels = data[\"type\"]\n",
    "\n",
    "seed = 1789\n",
    "X_train_val, X_val_test, y_train_val, y_val_test = model_selection.train_test_split(features, labels, test_size=0.3, random_state=seed)\n",
    "X_val, X_test, y_val, y_test = model_selection.train_test_split(X_val_test, y_val_test, test_size=0.5)\n",
    "X_train, X_prune, y_train, y_prune = model_selection.train_test_split(X_train_val, y_train_val, test_size=0.20)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, limit=None, left=None, right=None, ig=None, value=None, parent=None, majority=None, visited=False):\n",
    "        self.feature = feature\n",
    "        self.limit = limit\n",
    "        self.left= left\n",
    "        self.right = right\n",
    "        self.ig = ig\n",
    "        self.value = value\n",
    "        self.parent = parent\n",
    "        self.majority = majority # most 1 or 0\n",
    "        self.visited = visited\n",
    "        \n",
    "    def is_leaf(self):\n",
    "        return self.left is None and self.right is None\n",
    "\n",
    "\n",
    "def same_label(labels):\n",
    "    return labels.iloc[0]\n",
    "\n",
    "\n",
    "def entropy_calculator(labels):\n",
    "    tot = len(labels)\n",
    "    if not tot == 0: \n",
    "        a = labels.eq(1).sum() / tot \n",
    "        b = labels.eq(0).sum() / tot \n",
    "        h = -1 * ((a * np.log2(a)) + (b * np.log2(b)))\n",
    "    else:\n",
    "        h = 0\n",
    "    return h\n",
    "\n",
    "\n",
    "def find_limit(feature):\n",
    "    return feature.mean()\n",
    "\n",
    "\n",
    "def conditional_calculator(features, labels, impurity):\n",
    "    feature_values = []\n",
    "\n",
    "    for feature in features:\n",
    "        limit = find_limit(features[feature])\n",
    "        over = labels[features[feature] >= limit]\n",
    "        under = labels[features[feature] < limit]\n",
    "\n",
    "        p_over = len(over) / len(labels) \n",
    "        p_under = len(under) / len(labels)\n",
    "        \n",
    "        values_over = 0\n",
    "        values_under = 0\n",
    "        \n",
    "        if len(over.unique()) == 1:\n",
    "            pass\n",
    "        elif len(under.unique()) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            if impurity == \"entropy\":\n",
    "                values_over = entropy_calculator(over) \n",
    "                values_under = entropy_calculator(under)\n",
    "            elif impurity == \"gini\":\n",
    "                values_over = gini_calculator(over) \n",
    "                values_under = gini_calculator(under)\n",
    "\n",
    "        conditional_value = (p_over * values_over) + (p_under * values_under)\n",
    "        feature_values.append([feature, conditional_value])\n",
    "\n",
    "    return feature_values\n",
    "\n",
    "\n",
    "def gini_calculator(labels):    \n",
    "    tot = len(labels)\n",
    "    if not tot == 0:    \n",
    "        a = (labels.eq(1).sum())/len(labels)\n",
    "        b = (labels.eq(0).sum())/len(labels)\n",
    "        h = 1 - (a**2 + b**2) \n",
    "    else:\n",
    "        h = 0\n",
    "    return h\n",
    "\n",
    "\n",
    "def highest_information_gain(feature_value, overall_value):\n",
    "    highest_IG = [\"0\", 0]\n",
    "    for feature, value in feature_value:\n",
    "        ig = overall_value - value \n",
    "        if ig >= highest_IG[1]:\n",
    "            highest_IG = [feature, ig]\n",
    "    highest_feature = highest_IG[0]\n",
    "    return highest_feature\n",
    "    \n",
    "\n",
    "def split_dataset(highest_IG, limit, new_features, new_labels):\n",
    "    condition_X1 = new_features[highest_IG] <= limit\n",
    "    condition_X2 = new_features[highest_IG] > limit\n",
    "    \n",
    "    X1 = new_features[condition_X1] \n",
    "    X2 = new_features[condition_X2] \n",
    "\n",
    "    y1 = new_labels[condition_X1] \n",
    "    y2 = new_labels[condition_X2] \n",
    "\n",
    "    return X1, X2, y1, y2\n",
    "\n",
    "\n",
    "def impurity_type(impurity_measure, X, y):\n",
    "    if impurity_measure == 'entropy':\n",
    "        entropy = entropy_calculator(y) \n",
    "        conditional_entropies = conditional_calculator(X, y, 'entropy') \n",
    "        highest_IG = highest_information_gain(conditional_entropies, entropy) \n",
    "        limit = find_limit(X[highest_IG])\n",
    "        X1, X2, y1, y2 = split_dataset(highest_IG, limit, X, y) \n",
    "        majority = y.mode()[0]\n",
    "        return X1, X2, y1, y2, highest_IG, limit, majority\n",
    "    \n",
    "    elif impurity_measure == 'gini':\n",
    "        overall_gini = gini_calculator(y)\n",
    "        gini_index = conditional_calculator(X, y, 'gini')        \n",
    "        highest_IG = highest_information_gain(gini_index, overall_gini)\n",
    "        limit = find_limit(X[highest_IG])\n",
    "        X1, X2, y1, y2 = split_dataset(highest_IG, limit, X, y)\n",
    "        majority = y.mode()[0]\n",
    "        return X1, X2, y1, y2, highest_IG, limit, majority\n",
    "        \n",
    "    else:\n",
    "        print(\"Please choose a valid impurity measure\")\n",
    "    \n",
    "    \n",
    "def check_features(features):\n",
    "    for feature in features:\n",
    "        limit = find_limit(features[feature])\n",
    "        over = []\n",
    "        under = []\n",
    "        for value in features[feature]:\n",
    "            if value >= limit:\n",
    "                over.append(1)\n",
    "            else:\n",
    "                under.append(0)\n",
    "        if len(over) == 0 or len(under) == 0:\n",
    "            return True\n",
    "        else: \n",
    "            return False\n",
    "\n",
    "\n",
    "            \n",
    "# 1) learn()\n",
    "\n",
    "def learn(X, y, impurity_measure): \n",
    "    if len(y.unique()) == 1:\n",
    "        leaf = same_label(y) \n",
    "        return Node(value=leaf)\n",
    "    \n",
    "    elif check_features(X): \n",
    "        leaf = y.mode()[0]\n",
    "        return Node(value = leaf)\n",
    "    else: \n",
    "        X1, X2, y1, y2, highest_IG, limit, majority = impurity_type(impurity_measure, X, y)\n",
    "        learn_node_left = learn(X1,y1, impurity_measure)        \n",
    "        learn_node_right = learn(X2,y2, impurity_measure)\n",
    "        return Node(feature = highest_IG, limit = limit, left = learn_node_left, right = learn_node_right, majority = majority )\n",
    "\n",
    "    \n",
    "# 2) Predict()\n",
    "def _predict(x, node):\n",
    "    if node.is_leaf():\n",
    "        #y_pred.append(node.value)\n",
    "        return node.value\n",
    "    else:\n",
    "        if x[node.feature] < node.limit:\n",
    "            return _predict(x, node.left)\n",
    "    return _predict(x, node.right)\n",
    "\n",
    "\n",
    "def predict(X, root_node): \n",
    "        return [_predict(x, root_node)for index, x in X.iterrows()] \n",
    "    \n",
    "\n",
    "# Task 1.1 prediction with entropy:\n",
    "\n",
    "root_node_entropy = learn(X_train, y_train, impurity_measure='entropy')\n",
    "\n",
    "    \n",
    "# Task 1.2 prediction with gini index:\n",
    "\n",
    "root_node_gini = learn(X_train, y_train, impurity_measure='gini')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "42dd0f05-bcc2-410e-8aca-ee42cb1bf25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for entropy model: 0.86875\n",
      "Accuracy for gini model: 0.86875\n",
      "Accuracy after pruning entropy: 0.86875\n",
      "Accuracy after pruning entropy and gini: 0.86875\n",
      "Accuracy for entropy model: 0.5708333333333333\n"
     ]
    }
   ],
   "source": [
    "# Task 1.3 Add reduced-error pruning\n",
    "\n",
    "# 1) Full decision tree from training data:\n",
    "def print_decision_tree(node, indent=\"\"):\n",
    "    print(Node(node))\n",
    "    if node.is_leaf():\n",
    "        print(indent + \"Leaf Node: Value =\", node.value)\n",
    "    else:\n",
    "        print(indent + \"Split Node: Feature =\", node.feature)\n",
    "        print(indent + \"  Left Branch:\")\n",
    "        print_decision_tree(node.left, indent + \"   \")\n",
    "        print(indent + \"  Right Branch:\")\n",
    "        print_decision_tree(node.right, indent + \"   \")\n",
    "\n",
    "# Accuracy before pruning:\n",
    "\n",
    "accuracy_model = {}\n",
    "\n",
    "def accuracy(X, y_pred, root_node, measure):\n",
    "    y_pred_ = predict(X_val, root_node_gini)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracy_model[measure] = accuracy\n",
    "    print(\"Accuracy for\",measure,\"model:\", accuracy)\n",
    "\n",
    "    # Accuracy for model with entropy\n",
    "\n",
    "y_pred = predict(X_val, root_node_entropy)\n",
    "accuracy(X_val, y_pred, root_node_entropy, 'entropy')\n",
    "\n",
    "    # Accuracy for model with gini\n",
    "\n",
    "y_pred = predict(X_val, root_node_gini)\n",
    "accuracy(X_val, y_pred, root_node_gini, 'gini')\n",
    "\n",
    "def highest_accuracy(accuracy_model):\n",
    "    best_accuracy = 0\n",
    "    for measure in accuracy_model:\n",
    "        if accuracy_model[measure] > best_accuracy:\n",
    "            best_accuracy = accuracy_model[measure]\n",
    "    return best_accuracy\n",
    "\n",
    "\n",
    "# 2) Replace subtree T of T*:\n",
    "\n",
    "def prune(node, parent=None): \n",
    "    acc = highest_accuracy(accuracy_model)\n",
    "    node.visited = True\n",
    "    if parent is None:\n",
    "        pass\n",
    "    if node.is_leaf(): \n",
    "        \n",
    "        left = parent.left\n",
    "        right = parent.right\n",
    "        \n",
    "        parent.left = None \n",
    "        parent.right = None\n",
    "        parent.value = parent.majority\n",
    "\n",
    "        y_pred = predict(X_prune, node)\n",
    "            \n",
    "        accuracy = accuracy_score(y_prune, y_pred) \n",
    "        \n",
    "        if accuracy >= acc:\n",
    "            acc = accuracy\n",
    "        else:\n",
    "            parent.left = left\n",
    "            parent.right = right\n",
    "            parent.value = None \n",
    "    else: \n",
    "        prune(node.left, node) \n",
    "        prune(node.right, node)\n",
    "    return acc    \n",
    "\n",
    "# 1.4 Evaluate your algorithm \n",
    "\n",
    "accuracy_entropy_prune = prune(root_node_entropy)\n",
    "print('Accuracy after pruning entropy:',accuracy_entropy_prune)\n",
    "accuracy_gini_prune = prune(root_node_gini)\n",
    "print('Accuracy after pruning entropy and gini:',accuracy_gini_prune)\n",
    "\n",
    "# The accuracy of entropy is highest before and after pruning, our best model is entropy without pruning\n",
    "\n",
    "# Test the best model\n",
    "\n",
    "y_pred = predict(X_test, root_node_entropy)\n",
    "accuracy(X_test, y_test, root_node_entropy, 'entropy')\n",
    "\n",
    "# Test is low, and therefore the model is not as good as hoped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "95efd150-0539-4760-899c-1f0bf60bf476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Sklearn: 0.8875\n"
     ]
    }
   ],
   "source": [
    "# Task 1.5 Compare to an existing implementation \n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy for model from Sklearn\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Sklearn:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2354e5dd-d3b0-41f9-9585-5c29586088bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conlusion: sklearn is the best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
